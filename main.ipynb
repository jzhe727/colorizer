{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19752a7f-f2d2-49fb-821f-b22a9b86869f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from data.ipynb\n",
      "importing Jupyter notebook from CUnet.ipynb\n",
      "torch.Size([1, 1, 256, 256])\n",
      "torch.Size([1, 1, 128, 128])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "torch.Size([1, 2, 256, 256])\n",
      "importing Jupyter notebook from eccv16.ipynb\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]             640\n",
      "              ReLU-2         [-1, 64, 256, 256]               0\n",
      "            Conv2d-3         [-1, 64, 128, 128]          36,928\n",
      "              ReLU-4         [-1, 64, 128, 128]               0\n",
      "       BatchNorm2d-5         [-1, 64, 128, 128]             128\n",
      "            Conv2d-6        [-1, 128, 128, 128]          73,856\n",
      "              ReLU-7        [-1, 128, 128, 128]               0\n",
      "            Conv2d-8          [-1, 128, 64, 64]         147,584\n",
      "              ReLU-9          [-1, 128, 64, 64]               0\n",
      "      BatchNorm2d-10          [-1, 128, 64, 64]             256\n",
      "           Conv2d-11          [-1, 256, 64, 64]         295,168\n",
      "             ReLU-12          [-1, 256, 64, 64]               0\n",
      "           Conv2d-13          [-1, 256, 64, 64]         590,080\n",
      "             ReLU-14          [-1, 256, 64, 64]               0\n",
      "           Conv2d-15          [-1, 256, 32, 32]         590,080\n",
      "             ReLU-16          [-1, 256, 32, 32]               0\n",
      "      BatchNorm2d-17          [-1, 256, 32, 32]             512\n",
      "           Conv2d-18          [-1, 512, 32, 32]       1,180,160\n",
      "             ReLU-19          [-1, 512, 32, 32]               0\n",
      "           Conv2d-20          [-1, 512, 32, 32]       2,359,808\n",
      "             ReLU-21          [-1, 512, 32, 32]               0\n",
      "           Conv2d-22          [-1, 512, 32, 32]       2,359,808\n",
      "             ReLU-23          [-1, 512, 32, 32]               0\n",
      "      BatchNorm2d-24          [-1, 512, 32, 32]           1,024\n",
      "           Conv2d-25          [-1, 512, 32, 32]       2,359,808\n",
      "             ReLU-26          [-1, 512, 32, 32]               0\n",
      "           Conv2d-27          [-1, 512, 32, 32]       2,359,808\n",
      "             ReLU-28          [-1, 512, 32, 32]               0\n",
      "           Conv2d-29          [-1, 512, 32, 32]       2,359,808\n",
      "             ReLU-30          [-1, 512, 32, 32]               0\n",
      "      BatchNorm2d-31          [-1, 512, 32, 32]           1,024\n",
      "           Conv2d-32          [-1, 512, 32, 32]       2,359,808\n",
      "             ReLU-33          [-1, 512, 32, 32]               0\n",
      "           Conv2d-34          [-1, 512, 32, 32]       2,359,808\n",
      "             ReLU-35          [-1, 512, 32, 32]               0\n",
      "           Conv2d-36          [-1, 512, 32, 32]       2,359,808\n",
      "             ReLU-37          [-1, 512, 32, 32]               0\n",
      "      BatchNorm2d-38          [-1, 512, 32, 32]           1,024\n",
      "           Conv2d-39          [-1, 512, 32, 32]       2,359,808\n",
      "             ReLU-40          [-1, 512, 32, 32]               0\n",
      "           Conv2d-41          [-1, 512, 32, 32]       2,359,808\n",
      "             ReLU-42          [-1, 512, 32, 32]               0\n",
      "           Conv2d-43          [-1, 512, 32, 32]       2,359,808\n",
      "             ReLU-44          [-1, 512, 32, 32]               0\n",
      "      BatchNorm2d-45          [-1, 512, 32, 32]           1,024\n",
      "  ConvTranspose2d-46          [-1, 256, 64, 64]       2,097,408\n",
      "             ReLU-47          [-1, 256, 64, 64]               0\n",
      "           Conv2d-48          [-1, 256, 64, 64]         590,080\n",
      "             ReLU-49          [-1, 256, 64, 64]               0\n",
      "           Conv2d-50          [-1, 256, 64, 64]         590,080\n",
      "             ReLU-51          [-1, 256, 64, 64]               0\n",
      "           Conv2d-52          [-1, 313, 64, 64]          80,441\n",
      "          Softmax-53          [-1, 313, 64, 64]               0\n",
      "           Conv2d-54            [-1, 2, 64, 64]             626\n",
      "         Upsample-55          [-1, 2, 256, 256]               0\n",
      "================================================================\n",
      "Total params: 32,236,011\n",
      "Trainable params: 32,236,011\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 350.62\n",
      "Params size (MB): 122.97\n",
      "Estimated Total Size (MB): 473.85\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "import imutils\n",
    "from skimage import color\n",
    "from skimage.util.dtype import convert\n",
    "import import_ipynb #pip install import-ipynb\n",
    "\n",
    "from data import ImageNet, VisionDataset\n",
    "from CUnet import CUNet\n",
    "from eccv16 import eccv16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcf5105-55d4-43ca-a373-6e83621a3827",
   "metadata": {},
   "source": [
    "## Define models and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e97432a8-63f0-4359-a62e-78958ade4699",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cu_model = CUNet()\n",
    "ecc_model = eccv16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d81c6ba2-5ec3-4887-b17a-33f6371f1cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "ImageNet_train_dataset = ImageNet(root=\"./imagenet/train\", transform=trans, size=10)\n",
    "ImageNet_eval_dataset = ImageNet(root=\"./imagenet/val\", transform=trans, size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aa18c3b-81ec-4a8c-8dd1-740e215ac82a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader_imagenet = DataLoader(ImageNet_train_dataset, batch_size=32, shuffle=True)\n",
    "eval_dataloader_imagenet = DataLoader(ImageNet_eval_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e3d129-0394-4a16-a346-c9123e42d80a",
   "metadata": {},
   "source": [
    "## Define Hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67a3d7e5-e1ce-473a-9c00-1c5466872e97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.SGD(cu_model.parameters(), lr = 0.1) #change as needed\n",
    "loss_function = torch.nn.MSELoss() #change as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941fdb52-700a-4f49-becc-9b485a683f33",
   "metadata": {},
   "source": [
    "## Define Train and eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b404f87f-107b-4533-ab71-f0232172291c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(dataloader, model, device, optimizer, loss_function, epochs=1):\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_index,(inputs, expected) in enumerate(dataloader):\n",
    "            optimizer.zero_grad() \n",
    "            inputs = inputs.to(device)\n",
    "            expected = expected.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, expected)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*len(expected)\n",
    "        filename = f'model/{type(model).__name__}_{epoch}_{train_loss}.pth'\n",
    "        torch.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34fe5fb3-8e4c-40da-aee8-7bf03dab23b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(dataloader, model, device, optimizer, loss_function, epochs=1):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for batch_index,(inputs, expected) in enumerate(dataloader):\n",
    "        # inputs = torchvision.transforms.ToTensor(train_dataloader_imagenet.resize_image(lightness))\n",
    "        # expected = torchvision.transforms.ToTensor(train_dataloader_imagenet.resize_image(colors))\n",
    "        # inputs.type(torch.float32)\n",
    "        # expected.type(torch.float32)\n",
    "        print(type(inputs))\n",
    "        inputs = inputs.to(device)\n",
    "        expected = expected.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, expected)\n",
    "        val_loss += loss.item()*len(expected)\n",
    "    avg_val_loss = val_loss/len(eval_dataloader_imagenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dff1e82b-1de1-415c-af72-6e6f25939a05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def name_to_epoch(name):\n",
    "    _, a = name.split(\"/\")\n",
    "    _, epoch, _ = name.split(\"_\") #name, epoch, avg_l.pth\n",
    "    return(int(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "349a78f9-c003-4810-934a-119af7b0e726",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_model(train_dataloader_imagenet, cu_model, device, optimizer, loss_function, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ba56415-ee81-40b5-94e8-2ac0e6114b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:29: FutureWarning: The use of this function is discouraged as its behavior may change dramatically in scikit-image 1.0. This function will be removed in scikit-image 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 256, 256])\n",
      "torch.Size([10, 2, 256, 256])\n",
      "torch.Size([10, 2, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "train_model(train_dataloader_imagenet, ecc_model, device, optimizer, loss_function, 1) #upsample error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac991b0-1410-4068-a76b-54f8936890db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d58f436-eaa6-44d2-a0d3-272c2239b262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4256d494-aae0-451e-8ea4-a1b95ed02e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
