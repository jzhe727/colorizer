{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19752a7f-f2d2-49fb-821f-b22a9b86869f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "import imutils\n",
    "from skimage import color\n",
    "from skimage.util.dtype import convert\n",
    "import import_ipynb #pip install import-ipynb\n",
    "\n",
    "from data import ImageNet, VisionDataset\n",
    "from CUnet import CUNet\n",
    "from eccv16 import eccv16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e97432a8-63f0-4359-a62e-78958ade4699",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cu_model = CUNet()\n",
    "ecc_model = eccv16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d81c6ba2-5ec3-4887-b17a-33f6371f1cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "ImageNet_train_dataset = ImageNet(root=\"./imagenet/train\", transform=trans)\n",
    "ImageNet_eval_dataset = ImageNet(root=\"./imagenet/val\", transform=trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6aa18c3b-81ec-4a8c-8dd1-740e215ac82a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader_imagenet = DataLoader(ImageNet_train_dataset, batch_size=32, shuffle=True)\n",
    "eval_dataloader_imagenet = DataLoader(ImageNet_eval_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67a3d7e5-e1ce-473a-9c00-1c5466872e97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.SGD(cu_model.parameters(), lr = 0.1) #change as needed\n",
    "loss_function = torch.nn.MSELoss() #change as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b404f87f-107b-4533-ab71-f0232172291c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(dataloader, model, device, optimizer, loss_function, epochs=1):\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_index,(inputs, expected) in enumerate(dataloader):\n",
    "            optimizer.zero_grad() \n",
    "            inputs = inputs.to(device)\n",
    "            expected = expected.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, expected)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*len(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34fe5fb3-8e4c-40da-aee8-7bf03dab23b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(dataloader, model, device, optimizer, loss_function, epochs=1):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for batch_index,(inputs, expected) in enumerate(dataloader):\n",
    "        # inputs = torchvision.transforms.ToTensor(train_dataloader_imagenet.resize_image(lightness))\n",
    "        # expected = torchvision.transforms.ToTensor(train_dataloader_imagenet.resize_image(colors))\n",
    "        # inputs.type(torch.float32)\n",
    "        # expected.type(torch.float32)\n",
    "        print(type(inputs))\n",
    "        inputs = inputs.to(device)\n",
    "        expected = expected.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, expected)\n",
    "        val_loss += loss.item()*len(expected)\n",
    "    avg_val_loss = val_loss/len(eval_dataloader_imagenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dff1e82b-1de1-415c-af72-6e6f25939a05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def name_to_epoch(name):\n",
    "    _, a = name.split(\"/\")\n",
    "    _, epoch, _ = name.split(\"_\") #name, epoch, avg_l.pth\n",
    "    return(int(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "349a78f9-c003-4810-934a-119af7b0e726",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:29: FutureWarning: The use of this function is discouraged as its behavior may change dramatically in scikit-image 1.0. This function will be removed in scikit-image 1.0.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "the input array must have size 3 along `channel_axis`, got (256, 256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader_imagenet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 5\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(dataloader, model, device, optimizer, loss_function, epochs)\u001b[0m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      4\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_index,(inputs, expected) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m      6\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \n\u001b[1;32m      7\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m<string>:30\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/skimage/_shared/utils.py:326\u001b[0m, in \u001b[0;36mchannel_as_last_axis.__call__.<locals>.fixed_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m channel_axis \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_axis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m channel_axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# TODO: convert scalars to a tuple in anticipation of eventually\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m#       supporting a tuple of channel axes. Right now, only an\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m#       integer or a single-element tuple is supported, though.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(channel_axis):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/skimage/color/colorconv.py:1170\u001b[0m, in \u001b[0;36mrgb2lab\u001b[0;34m(rgb, illuminant, observer, channel_axis)\u001b[0m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;129m@channel_as_last_axis\u001b[39m()\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrgb2lab\u001b[39m(rgb, illuminant\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD65\u001b[39m\u001b[38;5;124m\"\u001b[39m, observer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m, channel_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Conversion from the sRGB color space (IEC 61966-2-1:1999)\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;124;03m    to the CIE Lab colorspace under the given illuminant and observer.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;124;03m    .. [1] https://en.wikipedia.org/wiki/Standard_illuminant\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xyz2lab(\u001b[43mrgb2xyz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb\u001b[49m\u001b[43m)\u001b[49m, illuminant, observer)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/skimage/_shared/utils.py:326\u001b[0m, in \u001b[0;36mchannel_as_last_axis.__call__.<locals>.fixed_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m channel_axis \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_axis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m channel_axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# TODO: convert scalars to a tuple in anticipation of eventually\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m#       supporting a tuple of channel axes. Right now, only an\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m#       integer or a single-element tuple is supported, though.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(channel_axis):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/skimage/color/colorconv.py:746\u001b[0m, in \u001b[0;36mrgb2xyz\u001b[0;34m(rgb, channel_axis)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"RGB to XYZ color space conversion.\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \n\u001b[1;32m    707\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;124;03m>>> img_xyz = rgb2xyz(img)\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    744\u001b[0m \u001b[38;5;66;03m# Follow the algorithm from http://www.easyrgb.com/index.php\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;66;03m# except we don't multiply/divide by 100 in the conversion\u001b[39;00m\n\u001b[0;32m--> 746\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_colorarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    747\u001b[0m mask \u001b[38;5;241m=\u001b[39m arr \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.04045\u001b[39m\n\u001b[1;32m    748\u001b[0m arr[mask] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpower((arr[mask] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.055\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1.055\u001b[39m, \u001b[38;5;241m2.4\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/skimage/color/colorconv.py:140\u001b[0m, in \u001b[0;36m_prepare_colorarray\u001b[0;34m(arr, force_copy, channel_axis)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mshape[channel_axis] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    138\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe input array must have size 3 along `channel_axis`, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    139\u001b[0m            \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marr\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    142\u001b[0m float_dtype \u001b[38;5;241m=\u001b[39m _supported_float_type(arr\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m float_dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32:\n",
      "\u001b[0;31mValueError\u001b[0m: the input array must have size 3 along `channel_axis`, got (256, 256)"
     ]
    }
   ],
   "source": [
    "train_model(train_dataloader_imagenet, cu_model, device, optimizer, loss_function, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e8e3d7-bdd9-40e8-96ff-5ebd7b90c70d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#filename = model/type(model)_epoch_avg_validation_loss.pth'\n",
    "\n",
    "model = cu_model\n",
    "def name_to_epoch(name):\n",
    "    _, a = name.split(\"/\")\n",
    "    _, epoch, _ = name.split(\"_\") #name, epoch, avg_l.pth\n",
    "    return(int(epoch))\n",
    "\n",
    "paths = glob.glob(f'model/{type(model).__name__}_*.pth')\n",
    "#only find models that have the same type as the model being trained\n",
    "\n",
    "paths.sort(key = name_to_epoch) #most recent epoch\n",
    "start = 0\n",
    "if len(paths) > 0:\n",
    "    target = paths[-1]\n",
    "    start = name_to_epoch(target)\n",
    "    model.load_state_dict(torch.load(target)) \n",
    "\n",
    "\n",
    "# model = model.to(device)\n",
    "\n",
    "for epoch in range(start, EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_index,(inputs, expected) in enumerate(train_dataloader_imagenet):\n",
    "        optimizer.zero_grad() \n",
    "        inputs = inputs.to(device)\n",
    "        expected = expected.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, expected)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*len(expected)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for batch_index,(inputs, expected) in enumerate(eval_dataloader_imagenet):\n",
    "        # inputs = torchvision.transforms.ToTensor(train_dataloader_imagenet.resize_image(lightness))\n",
    "        # expected = torchvision.transforms.ToTensor(train_dataloader_imagenet.resize_image(colors))\n",
    "        # inputs.type(torch.float32)\n",
    "        # expected.type(torch.float32)\n",
    "        print(type(inputs))\n",
    "        inputs = inputs.to(device)\n",
    "        expected = expected.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, expected)\n",
    "        val_loss += loss.item()*len(expected)\n",
    "    avg_val_loss = val_loss/len(eval_dataloader_imagenet)\n",
    "    filename = f'model/{type(model).__name__}_{epoch}_{avg_val_loss}.pth'\n",
    "    torch.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba56415-ee81-40b5-94e8-2ac0e6114b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac991b0-1410-4068-a76b-54f8936890db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d58f436-eaa6-44d2-a0d3-272c2239b262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4256d494-aae0-451e-8ea4-a1b95ed02e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
