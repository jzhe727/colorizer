{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "19752a7f-f2d2-49fb-821f-b22a9b86869f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "import imutils\n",
    "from skimage import color\n",
    "from skimage.util.dtype import convert\n",
    "import import_ipynb #pip install import-ipynb\n",
    "\n",
    "from data import ImageNet, VisionDataset\n",
    "from CUnet import CUNet\n",
    "from eccv16 import eccv16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcf5105-55d4-43ca-a373-6e83621a3827",
   "metadata": {},
   "source": [
    "## Define models and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e97432a8-63f0-4359-a62e-78958ade4699",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cu_model = CUNet()\n",
    "ecc_model = eccv16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d81c6ba2-5ec3-4887-b17a-33f6371f1cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "ImageNet_train_dataset = ImageNet(root=\"./imagenet/train\", transform=trans)\n",
    "ImageNet_eval_dataset = ImageNet(root=\"./imagenet/val\", transform=trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6aa18c3b-81ec-4a8c-8dd1-740e215ac82a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader_imagenet = DataLoader(ImageNet_train_dataset, batch_size=32, shuffle=True)\n",
    "eval_dataloader_imagenet = DataLoader(ImageNet_eval_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e3d129-0394-4a16-a346-c9123e42d80a",
   "metadata": {},
   "source": [
    "## Define Hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "67a3d7e5-e1ce-473a-9c00-1c5466872e97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.Adam(cu_model.parameters(), lr=0.001) #change as needed\n",
    "loss_function = torch.nn.MSELoss() #change as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941fdb52-700a-4f49-becc-9b485a683f33",
   "metadata": {},
   "source": [
    "## Define Train and eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b404f87f-107b-4533-ab71-f0232172291c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(dataloader, model, device, optimizer, loss_function, epochs=1, epoch_offset=0):\n",
    "    for epoch in range(epoch_offset+1, epochs+epoch_offset+1):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_index,(inputs, expected) in enumerate(dataloader):\n",
    "            optimizer.zero_grad() \n",
    "            inputs = inputs.to(device)\n",
    "            expected = expected.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, expected)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*len(expected)\n",
    "        print(\"Epoch\",epoch,\"Done\")\n",
    "        if epoch%2==0:\n",
    "            filename = f'model/{type(model).__name__}_{epoch}_{train_loss}.pth'\n",
    "            torch.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "34fe5fb3-8e4c-40da-aee8-7bf03dab23b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs to be fixed, has to calculate accuracy and print loss\n",
    "def eval_model(dataloader, model, device, loss_function):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for batch_index,(inputs, expected) in enumerate(dataloader):\n",
    "        # inputs = torchvision.transforms.ToTensor(train_dataloader_imagenet.resize_image(lightness))\n",
    "        # expected = torchvision.transforms.ToTensor(train_dataloader_imagenet.resize_image(colors))\n",
    "        # inputs.type(torch.float32)\n",
    "        # expected.type(torch.float32)\n",
    "        print(type(inputs))\n",
    "        inputs = inputs.to(device)\n",
    "        expected = expected.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, expected)\n",
    "        val_loss += loss.item()*len(expected)\n",
    "    avg_val_loss = val_loss/len(eval_dataloader_imagenet)\n",
    "    print(\"Eval Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b89b8478-e292-4b15-a162-eb74f839a3da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_comparisons(start,model):\n",
    "    for i in range(start,start+5):\n",
    "        org1, pred1 = ImageNet_train_dataset.check_output(i,model)\n",
    "        figure, axes = plt.subplots(1, 2, figsize=(18,10))\n",
    "        axes = axes.flatten()    \n",
    "        axes[0].imshow(org1)\n",
    "        axes[1].imshow(pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dff1e82b-1de1-415c-af72-6e6f25939a05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def name_to_epoch(name):\n",
    "    _, epoch, _ = name.split(\"_\") #name, epoch, avg_l.pth\n",
    "    return(int(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2853a116-64c2-43d9-9ceb-73712dc12b56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_latest_weights(model):\n",
    "    paths = glob.glob(f'model/{type(model).__name__}_*.pth')\n",
    "    #only find models that have the same type as the model being trained\n",
    "    \n",
    "    paths.sort(key = name_to_epoch) #most recent epoch\n",
    "    start = -1\n",
    "    if len(paths) > 0:\n",
    "        target = paths[-1]\n",
    "        start = name_to_epoch(target)\n",
    "        model.load_state_dict(torch.load(target))\n",
    "    return start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceadcf7-4237-4ffe-ad65-28fdd3a2b073",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bb4a79-318c-4018-aa2c-18308c02511f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = get_latest_weights(cu_model)\n",
    "train_model(train_dataloader_imagenet, cu_model, device, optimizer, loss_function, 40, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c4be5b-f03b-49ab-a726-cb7892c6a6b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = get_latest_weights(ecc_model)\n",
    "train_model(train_dataloader_imagenet, ecc_model, device, optimizer, loss_function, EPOCHS, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dff0c8-c4b6-4aca-8d4c-47bc9e0507ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed16b39-b2b0-4bb7-bfc2-e665282699c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "629fbedd-206a-48f2-b584-95b8eeadaf67",
   "metadata": {},
   "source": [
    "### Eval Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d9e9d3-bcdd-4877-a936-c2cf3a57d805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_model(eval_dataloader_imagenet, cu_model, device, loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0f9801-0e4b-475d-ae2d-1eb3794571d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_model(eval_dataloader_imagenet, ecc_model, device, loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f96987-8a05-40d5-9dc7-e10fd05b28cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_comparisons(10,cu_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cc03ef-8485-430f-9429-f07a34e03cab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb389127-fe68-4147-94fc-9713894c07d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
