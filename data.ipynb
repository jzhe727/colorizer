{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a755e1d-2046-41ca-a20f-3dfe49f43a5a",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "b247cb7d-52f1-4436-a099-ff1eb4099f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "import imutils\n",
    "from skimage import color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "47202977-5ce1-4634-8994-0ab4f254381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another dataset class incase mflickr is not enough data, based on a subset of Imagenet\n",
    "class ImageNet(Dataset):\n",
    "    '''\n",
    "    Constructor \n",
    "    Inputs:\n",
    "        root -> root dir of images,\n",
    "        ext -> optional extension of images, default jpg\n",
    "        size -> optional, crops data size for faster testing\n",
    "    '''\n",
    "    def __init__(self, root, ext='.JPEG', size=None, transform=None):\n",
    "        self.paths = glob.glob(f'{root}/*{ext}', recursive=True)\n",
    "        self.size = size\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.height = 256\n",
    "        self.width = 256\n",
    "        \n",
    "        if size: #speed up testing\n",
    "            self.paths = self.paths[:size]\n",
    "    \n",
    "    '''\n",
    "    get the img at index i\n",
    "    '''\n",
    "    def __getitem__(self, i):\n",
    "        s = (self.paths[i])\n",
    "        img = Image.open(s)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "    \n",
    "    '''\n",
    "    return length of dataset\n",
    "    '''\n",
    "    def __len__(self):\n",
    "        return(len(self.paths))\n",
    "    \n",
    "    '''\n",
    "    helper function that takes in original image, and returns \n",
    "    original size and resize grayscale (L) dimension\n",
    "    '''\n",
    "    def preprocess_img(self, img_rgb_orig, resample=3):\n",
    "        img_rgb_rs = self.resize_img(img_rgb_orig) # original image resized\n",
    "        \n",
    "        img_lab_orig = color.rgb2lab(img_rgb_orig) # original image to lab space\n",
    "        img_lab_rs = color.rgb2lab(img_rgb_rs)     # resized lab image \n",
    "        \n",
    "        img_l_orig = img_lab_orig[:,:,0]           # original size L channel \n",
    "        img_l_rs = img_lab_rs[:,:,0]               # resize L channel\n",
    "    \n",
    "        tens_orig_l = torch.Tensor(img_l_orig)[None,:,:] # to tensor\n",
    "        tens_rs_l = torch.Tensor(img_l_rs)[None,:,:]\n",
    "    \n",
    "        return (tens_orig_l, tens_rs_l)  # [1,256,256] for resized\n",
    "    \n",
    "    '''\n",
    "    rebuilds an image from the original L channel and the genereted ab output from the network\n",
    "    first concats the L channel on top of a,b then permutes it and then converts the lab back to rgb\n",
    "    '''\n",
    "    def postprocess_img(self, tens_orig_l, out_ab):\n",
    "        out_shape = out_ab.shape[1:]\n",
    "        orig_shape = tens_orig_l.shape[1:]\n",
    "        \n",
    "        if(out_shape != orig_shape):\n",
    "            out_ab = F.interpolate(out_ab, size=orig_shape, mode='bilinear')\n",
    "            \n",
    "        concat = torch.cat((tens_orig_l, out_ab), dim=0) # stack l on top of a,b\n",
    "        permute = concat.permute(1,2,0)                  # permute the image [1,256,256] -> [256,256,1]\n",
    "        rebuilt_image = color.lab2rgb(permute)           # convert to rgb\n",
    "        \n",
    "        return rebuilt_image        \n",
    "\n",
    "    '''\n",
    "    This function takes in an image and resizes it to 256 by 256\n",
    "    '''\n",
    "    def resize_img(self, img):\n",
    "        return (img.resize((self.height, self.width), resample=3))\n",
    "    \n",
    "    '''\n",
    "    Selects random start point in the dataset and prints 7 images\n",
    "    Inputs:\n",
    "        color-> 1 to print color images, 0 for grayscale images\n",
    "    '''\n",
    "    def print_samples(self):\n",
    "        figure, axes = plt.subplots(1, 7, figsize=(18,10))\n",
    "        axes = axes.flatten()\n",
    "        max_idx = self.size if self.size!=None else len(self.paths)\n",
    "        i = random.randint(0, max_idx)\n",
    "        for axis in axes:\n",
    "            x = self[i]\n",
    "            axis.imshow(x)\n",
    "            i+=1\n",
    "        plt.show()\n",
    "    \n",
    "    '''\n",
    "    prints all three LAB channels of a given image index.\n",
    "    also demonstrates how to rebuild original image from l,a,b channels\n",
    "    '''\n",
    "    def print_lab_channels(self,index):\n",
    "        img = self[index]\n",
    "        img_lab = color.rgb2lab(img)\n",
    "        l = torch.Tensor(img_lab[:,:,0])[None,:,:]\n",
    "        a = torch.Tensor(img_lab[:,:,1])[None,:,:]\n",
    "        b = torch.Tensor(img_lab[:,:,2])[None,:,:]\n",
    "        figure, axes = plt.subplots(1, 5, figsize=(18,10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        # Plot lab iamge and all 3 channels seperately\n",
    "        axes[0].imshow(img_lab) # currently weird colors because img_lab contains negative values\n",
    "        axes[0].set_xlabel(\"LAB image\")\n",
    "        axes[1].imshow(l.permute(1,2,0),cmap=\"gray\")\n",
    "        axes[1].set_xlabel(\"L Channel\")\n",
    "        axes[2].imshow(a.permute(1,2,0))\n",
    "        axes[2].set_xlabel(\"A Channel\")\n",
    "        axes[3].imshow(b.permute(1,2,0))\n",
    "        axes[3].set_xlabel(\"B Channel\")\n",
    "        \n",
    "        # Rebuilding original image from l,a,b layers\n",
    "        orig = torch.cat((l, a, b), dim=0)\n",
    "        orig = orig.permute(1,2,0)\n",
    "        orig = color.lab2rgb(orig)\n",
    "        axes[4].imshow(orig)\n",
    "        axes[4].set_xlabel(\"Rebuilt image\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    '''\n",
    "    takes an index and prints the original plus the L channel image\n",
    "    side by side\n",
    "    '''\n",
    "    def print_side_by_side(self,i):\n",
    "        figure, axes = plt.subplots(1, 2, figsize=(18,10))\n",
    "        axes = axes.flatten()\n",
    "        img = temp[i]\n",
    "        l_org, l_rs = preprocess_img(img)\n",
    "        l_org = l_org.permute(1,2,0)\n",
    "        axes[0].imshow(img)\n",
    "        axes[1].imshow(l_org, cmap=\"gray\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    '''\n",
    "    Calculates average width and height of all images in dataset\n",
    "    '''\n",
    "    def calc_average_dimension(self):\n",
    "        totalw = 0\n",
    "        totalh = 0\n",
    "        for i in range(len(self.paths)):\n",
    "            image = Image.open(self.paths[i])\n",
    "            w, h = image.size\n",
    "            totalw += w\n",
    "            totalh += h\n",
    "        avgw = totalw//len(self.paths)\n",
    "        avgh = totalh//len(self.paths)\n",
    "        return avgw, avgh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "1893cc5b-1b21-4ffd-baca-57defbed2aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ImageNet_train_dataset = ImageNet(root=\"./imagenet/train\")\n",
    "ImageNet_eval_dataset = ImageNet(root=\"./imagenet/val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "a2ce8d6d-da5c-4da0-b98a-bfb75c30b737",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader_imagenet = DataLoader(ImageNet_train_dataset, batch_size=32, shuffle=True)\n",
    "eval_dataloader_iamgenet = DataLoader(ImageNet_eval_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d7538a-5b00-452b-9705-03698dfba546",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Testing LAB conversion for ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "bf160915-8d43-4be0-b98c-66cbcd25457a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp = ImageNet(root=\"./imagenet/train\",size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "887bed89-0ca8-42e5-9e0d-1168672faa7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# temp.print_lab_channels(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "5f4c4218-3d13-4ce8-a49b-dda7c1cc3182",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# temp.print_lab_channels(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41e5d59-8364-4d38-992d-df5908c8bc29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cde510-164e-42e2-9c6a-dd7fc871bd0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cea582ef-edee-4a54-8f45-00a3b3b2488d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Class for the mirFlickr dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "cb1e0a75-b3c4-4082-a641-c3866415a453",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This code was based on the FlowerDataset class we did in Assignment 2\n",
    "class VisionDataset(Dataset):\n",
    "    '''\n",
    "    Constructor \n",
    "    Inputs:\n",
    "        root -> root dir of images,\n",
    "        ext -> optional extension of images, default jpg\n",
    "        size -> optional, crops data size for faster testing\n",
    "    '''\n",
    "    def __init__(self, root, ext='.jpg', size=None):\n",
    "        self.paths = glob.glob(f'{root}/*{ext}', recursive=True)\n",
    "        self.dataset = []         # array of tuples (black/white , original)\n",
    "        self.size = size\n",
    "        \n",
    "        if size: #speed up testing\n",
    "            self.paths = self.paths[:size]\n",
    "\n",
    "    '''\n",
    "    return length of dataset\n",
    "    '''\n",
    "    def __len__(self):\n",
    "        return(len(self.paths))\n",
    "    \n",
    "    '''\n",
    "    get the img,label tuple corresponding to index i\n",
    "    '''\n",
    "    def __getitem__(self, i):\n",
    "        s = (self.root+'/'+self.paths[i])\n",
    "        img = Image.open(s)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Selects random start point in the dataset and prints 7 images\n",
    "    Inputs:\n",
    "        color-> 1 to print color images, 0 for grayscale images\n",
    "    '''\n",
    "    def print_samples(self, color:int):\n",
    "        figure, axes = plt.subplots(1, 7, figsize=(18,10))\n",
    "        axes = axes.flatten()\n",
    "        i = random.randint(0, self.size-29)\n",
    "        \n",
    "        for axis in axes:\n",
    "            x = self.dataset[i]\n",
    "            if color:\n",
    "                axis.imshow(x[1])\n",
    "            else:\n",
    "                axis.imshow(x[0],cmap=\"gray\")\n",
    "            label = self.paths[i]\n",
    "            axis.set_xlabel(label)\n",
    "            i+=1\n",
    "        plt.show()\n",
    "    \n",
    "    '''\n",
    "    Calculates average width and height of all images in dataset\n",
    "    '''\n",
    "    def calc_average_dimension(self):\n",
    "        totalw = 0\n",
    "        totalh = 0\n",
    "        for i in range(len(self.paths)):\n",
    "            image = Image.open(self.paths[i])\n",
    "            w, h = image.size\n",
    "            totalw += w\n",
    "            totalh += h\n",
    "        avgw = totalw//len(self.paths)\n",
    "        avgh = totalh//len(self.paths)\n",
    "        return avgw, avgh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "db7b7a48-68c6-4d41-ad47-b5925dff3cf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = VisionDataset(\"./mirflickr\", ext=\".jpg\", size=1000)\n",
    "train_dataloader_vision = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1825d0-f5a3-4410-91c5-6cef466f02db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625ebfc1-32aa-4ccc-bca6-729b25b6652c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcc3708-badb-400a-bd66-1d57b8c605bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2a3d34-ee12-4871-9fa8-4010e536d5d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f35b38-9504-4718-9090-d2c99aa3284b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962aaf01-83e0-4ae7-85a6-71e88b982aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29846bf-f176-4aa6-9b99-d037ebaa4fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d22d2-2be6-4734-b288-c4558e641ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
