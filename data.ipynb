{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a755e1d-2046-41ca-a20f-3dfe49f43a5a",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b247cb7d-52f1-4436-a099-ff1eb4099f80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensor\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "import imutils\n",
    "from skimage import color\n",
    "from skimage.util.dtype import convert\n",
    "import import_ipynb #pip install import-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "47202977-5ce1-4634-8994-0ab4f254381d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Another dataset class incase mflickr is not enough data, based on a subset of Imagenet\n",
    "class ImageNet(Dataset):\n",
    "    '''\n",
    "    Constructor \n",
    "    Inputs:\n",
    "        root -> root dir of images,\n",
    "        ext -> optional extension of images, default jpg\n",
    "        size -> optional, crops data size for faster testing\n",
    "    '''\n",
    "    def __init__(self, root, ext='.JPEG', size=None, transform=None):\n",
    "        self.paths = glob.glob(f'{root}/*{ext}', recursive=True)\n",
    "        self.size = size\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.height = 256\n",
    "        self.width = 256\n",
    "        \n",
    "        if size: #speed up testing\n",
    "            self.paths = self.paths[:size]\n",
    "    \n",
    "    '''\n",
    "    get the (l,ab) pair for image i\n",
    "    '''\n",
    "    def __getitem__(self, i):\n",
    "        s = self.paths[i]\n",
    "        img = (Image.open(s)).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = img.resize((self.height, self.width))\n",
    "            img = convert(img, np.float32) # [256,256,3]\n",
    "            img = color.rgb2lab(img)\n",
    "            img = self.transform(img) # [3, 256, 256]\n",
    "            \n",
    "        l = (img[0,:,:])[None,:,:]\n",
    "        ab = img[1:,:,:]\n",
    "        return l, ab\n",
    "    \n",
    "    '''\n",
    "    get the l,ab pair for image i, no transforms\n",
    "    '''\n",
    "    def get_original(self, i):\n",
    "        s = self.paths[i]\n",
    "        img = (Image.open(s)).convert('RGB')\n",
    "        img = color.rgb2lab(img)\n",
    "        img = self.transform(img) # [3, 256, 256]\n",
    "            \n",
    "        l = (img[0,:,:])[None,:,:]\n",
    "        ab = img[1:,:,:]\n",
    "        return l, ab\n",
    "    \n",
    "    '''\n",
    "    return length of dataset\n",
    "    '''\n",
    "    def __len__(self):\n",
    "        return(len(self.paths))\n",
    "    \n",
    "    '''\n",
    "    Input:\n",
    "        tens_orig_l -> [batch_size, 1, height, width]\n",
    "        out_ab      -> [batch_size, 2, 256,    256]\n",
    "    tens_orig_l is the original images L dimension that is obtained from pre processing\n",
    "    out_ab is the 2-channel prediction outputted by the model\n",
    "    '''\n",
    "    def postprocess_batch(tens_orig_l, out_ab):\n",
    "        out_shape = out_ab.shape[2:]\n",
    "        orig_shape = tens_orig_l.shape[2:]\n",
    "        \n",
    "        # resize the model prediction to original image size using interpolate\n",
    "        if(out_shape!=orig_shape):\n",
    "            out_ab_orig = F.interpolate(out_ab, size=orig_shape, mode='bilinear')\n",
    "        \n",
    "        concat = torch.cat((tens_orig_l, out_ab_orig), dim=1) # [batch_size, 3, height, width]\n",
    "        return concat \n",
    "    \n",
    "    '''\n",
    "    This function takes in an image and resizes it to 256 by 256\n",
    "    '''\n",
    "    def resize_img(self, img):\n",
    "        return (img.resize((self.height, self.width)))\n",
    "    \n",
    "    '''\n",
    "    take a random index i, get the original image, and the predicted image\n",
    "    print them side by side\n",
    "    '''\n",
    "    def check_output(self, i, model):\n",
    "        l_org, ab_org = self.get_original[i]\n",
    "        img_original = self.rebuild_image(l_org, ab_org)\n",
    "        \n",
    "        l,ab = self[i]\n",
    "        pred_ab = model(l)\n",
    "        img_pred = self.rebuild_iamge(l,pred_ab) # needs to be upsized with post_process function still\n",
    "        return img_original, img_pred\n",
    "        \n",
    "    '''\n",
    "    Selects random start point in the dataset and prints 7 images\n",
    "    '''\n",
    "    def print_samples(self):\n",
    "        figure, axes = plt.subplots(1, 7, figsize=(18,10))\n",
    "        axes = axes.flatten()\n",
    "        max_idx = self.size if self.size!=None else len(self.paths)\n",
    "        i = random.randint(0, max_idx)\n",
    "        for axis in axes:\n",
    "            l, ab = self[i]\n",
    "            img = self.rebuild_image(l,ab)\n",
    "            axis.imshow(img)\n",
    "            axis.set_xlabel(i)\n",
    "            i+=1\n",
    "        plt.show()\n",
    "    \n",
    "    '''\n",
    "    prints all three LAB channels of a given image index.\n",
    "    also demonstrates how to rebuild original image from l,a,b channels\n",
    "    '''\n",
    "    def print_lab_channels(self,index):\n",
    "        img_l, img_ab = self[index]\n",
    "        \n",
    "        img_lab = torch.concat((img_l,img_ab), dim=0).permute(1,2,0)\n",
    "        l = torch.Tensor(img_l)\n",
    "        a = torch.Tensor(img_ab[0,:,:])[None,:,:]\n",
    "        b = torch.Tensor(img_ab[1,:,:])[None,:,:]\n",
    "        \n",
    "        figure, axes = plt.subplots(1, 5, figsize=(18,10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        # Plot lab iamge and all 3 channels seperately\n",
    "        axes[0].imshow(img_lab) # currently weird colors because img_lab contains negative values\n",
    "        axes[0].set_xlabel(\"LAB image\")\n",
    "        axes[1].imshow(l.permute(1,2,0),cmap=\"gray\")\n",
    "        axes[1].set_xlabel(\"L Channel\")\n",
    "        axes[2].imshow(a.permute(1,2,0))\n",
    "        axes[2].set_xlabel(\"A Channel\")\n",
    "        axes[3].imshow(b.permute(1,2,0))\n",
    "        axes[3].set_xlabel(\"B Channel\")\n",
    "        \n",
    "        orig = self.rebuild_image(img_l,img_ab)\n",
    "        axes[4].imshow(orig)\n",
    "        axes[4].set_xlabel(\"Rebuilt image\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    '''\n",
    "    takes an index and prints the original plus the L channel image\n",
    "    side by side\n",
    "    '''\n",
    "    def print_side_by_side(self,i):\n",
    "        l,ab = self[i]\n",
    "        \n",
    "        img = self.rebuild_image(l,ab)\n",
    "        l = l.permute(1,2,0) # original size l\n",
    "\n",
    "        figure, axes = plt.subplots(1, 2, figsize=(18,10))\n",
    "        axes = axes.flatten()\n",
    "        axes[0].imshow(img)\n",
    "        axes[1].imshow(l, cmap=\"gray\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    '''\n",
    "    rebuilds original rgb image from l,ab inputs\n",
    "    '''\n",
    "    def rebuild_image(self, l, ab):\n",
    "        return color.lab2rgb(torch.cat((l,ab), dim=0).permute(1,2,0))\n",
    "    \n",
    "    '''\n",
    "    Calculates average width and height of all images in dataset\n",
    "    '''\n",
    "    def calc_average_dimension(self):\n",
    "        totalw = 0\n",
    "        totalh = 0\n",
    "        for i in range(len(self.paths)):\n",
    "            image = Image.open(self.paths[i])\n",
    "            w, h = image.size\n",
    "            totalw += w\n",
    "            totalh += h\n",
    "        avgw = totalw//len(self.paths)\n",
    "        avgh = totalh//len(self.paths)\n",
    "        return avgw, avgh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1893cc5b-1b21-4ffd-baca-57defbed2aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "ImageNet_train_dataset = ImageNet(root=\"./imagenet/train\", transform=trans)\n",
    "ImageNet_eval_dataset = ImageNet(root=\"./imagenet/val\", transform=trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a2ce8d6d-da5c-4da0-b98a-bfb75c30b737",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader_imagenet = DataLoader(ImageNet_train_dataset, batch_size=32, shuffle=True)\n",
    "eval_dataloader_imagenet = DataLoader(ImageNet_eval_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d7538a-5b00-452b-9705-03698dfba546",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Testing LAB conversion for ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "bf160915-8d43-4be0-b98c-66cbcd25457a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_train = ImageNet(root=\"./imagenet/train\", transform=trans, size=20)\n",
    "temp_eval = ImageNet(root=\"./imagenet/val\", transform=trans, size=20)\n",
    "train_dataloader_imagenet2 = DataLoader(temp_train, batch_size=8, shuffle=True)\n",
    "eval_dataloader_imagenet2 = DataLoader(temp_eval, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea582ef-edee-4a54-8f45-00a3b3b2488d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Class for the mirFlickr dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "cb1e0a75-b3c4-4082-a641-c3866415a453",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This code was based on the FlowerDataset class we did in Assignment 2\n",
    "class VisionDataset(Dataset):\n",
    "    '''\n",
    "    Constructor \n",
    "    Inputs:\n",
    "        root -> root dir of images,\n",
    "        ext -> optional extension of images, default jpg\n",
    "        size -> optional, crops data size for faster testing\n",
    "    '''\n",
    "    def __init__(self, root, ext='.jpg', size=None, transform=None):\n",
    "        self.root = root\n",
    "        self.paths = glob.glob(f'{root}/*{ext}', recursive=True)\n",
    "        self.dataset = []         # array of tuples (black/white , original)\n",
    "        self.size = size if size else len(self.paths)\n",
    "        self.transform = transform\n",
    "        \n",
    "\n",
    "    '''\n",
    "    return length of dataset\n",
    "    '''\n",
    "    def __len__(self):\n",
    "        return(len(self.paths))\n",
    "    \n",
    "    '''\n",
    "    get the img,label tuple corresponding to index i\n",
    "    '''\n",
    "    def __getitem__(self, i):\n",
    "        s = (self.paths[i])\n",
    "        img = Image.open(s)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Selects random start point in the dataset and prints 7 images\n",
    "    Inputs:\n",
    "        color-> 1 to print color images, 0 for grayscale images\n",
    "    '''\n",
    "    def print_samples(self, color:int):\n",
    "        figure, axes = plt.subplots(1, 7, figsize=(18,10))\n",
    "        axes = axes.flatten()\n",
    "        i = random.randint(0, self.size-1000)\n",
    "        print(i, self.size)\n",
    "\n",
    "        for axis in axes:\n",
    "            x = self[i]\n",
    "            if color:\n",
    "                axis.imshow(x)\n",
    "            else:\n",
    "                axis.imshow(x,cmap=\"gray\")\n",
    "            label = self.paths[i]\n",
    "            axis.set_xlabel(label)\n",
    "            i+=1\n",
    "        plt.show()\n",
    "    \n",
    "    '''\n",
    "    Calculates average width and height of all images in dataset\n",
    "    '''\n",
    "    def calc_average_dimension(self):\n",
    "        totalw = 0\n",
    "        totalh = 0\n",
    "        for i in range(len(self.paths)):\n",
    "            image = Image.open(self.paths[i])\n",
    "            w, h = image.size\n",
    "            totalw += w\n",
    "            totalh += h\n",
    "        avgw = totalw//len(self.paths)\n",
    "        avgh = totalh//len(self.paths)\n",
    "        return avgw, avgh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "db7b7a48-68c6-4d41-ad47-b5925dff3cf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flickr_dataset = VisionDataset(\"./mirflickr\", ext=\".jpg\")\n",
    "train_dataloader_vision = DataLoader(flickr_dataset, batch_size=32, shuffle=True)\n",
    "validation_dataloader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29846bf-f176-4aa6-9b99-d037ebaa4fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d22d2-2be6-4734-b288-c4558e641ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
