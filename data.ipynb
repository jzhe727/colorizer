{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "6a755e1d-2046-41ca-a20f-3dfe49f43a5a",
            "metadata": {},
            "source": [
                "### Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "b247cb7d-52f1-4436-a099-ff1eb4099f80",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\johnz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torchvision\n",
                "from torchvision import datasets\n",
                "from torch.utils.data import Dataset, DataLoader, random_split\n",
                "import numpy as np\n",
                "from torchvision import datasets, transforms\n",
                "from PIL import Image, ImageOps\n",
                "import matplotlib.pyplot as plt\n",
                "import glob\n",
                "import random\n",
                "import cv2\n",
                "import imutils\n",
                "from skimage import color\n",
                "from skimage.util.dtype import convert\n",
                "import import_ipynb #pip install import-ipynb"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "47202977-5ce1-4634-8994-0ab4f254381d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Another dataset class incase mflickr is not enough data, based on a subset of Imagenet\n",
                "class ImageNet(Dataset):\n",
                "    '''\n",
                "    Constructor \n",
                "    Inputs:\n",
                "        root -> root dir of images,\n",
                "        ext -> optional extension of images, default jpg\n",
                "        size -> optional, crops data size for faster testing\n",
                "    '''\n",
                "    def __init__(self, root, ext='.JPEG', size=None, transform=None):\n",
                "        self.paths = glob.glob(f'{root}/*{ext}', recursive=True)\n",
                "        self.size = size\n",
                "        self.root = root\n",
                "        self.transform = transform\n",
                "        self.height = 256\n",
                "        self.width = 256\n",
                "        \n",
                "        if size: #speed up testing\n",
                "            self.paths = self.paths[:size]\n",
                "    \n",
                "    '''\n",
                "    get the img at index i\n",
                "    '''\n",
                "    def __getitem__(self, i):\n",
                "        s = self.paths[i]\n",
                "        img = Image.open(s)\n",
                "        if self.transform:\n",
                "            img = img.resize((self.height, self.width))\n",
                "            img = convert(img, np.float32)\n",
                "            img = color.rgb2lab(img)\n",
                "            img = self.transform(img)\n",
                "            \n",
                "        l = (img[0,:,:])[None,:,:]\n",
                "        ab = img[1:,:,:]\n",
                "        \n",
                "        return l, ab\n",
                "    \n",
                "    '''\n",
                "    return length of dataset\n",
                "    '''\n",
                "    def __len__(self):\n",
                "        return(len(self.paths))\n",
                "    \n",
                "    '''\n",
                "    Input:\n",
                "        tens_orig_l -> [batch_size, 1, height, width]\n",
                "        out_ab      -> [batch_size, 2, 256,    256]\n",
                "    tens_orig_l is the original images L dimension that is obtained from pre processing\n",
                "    out_ab is the 2-channel prediction outputted by the model\n",
                "    '''\n",
                "    def postprocess_batch(tens_orig_l, out_ab):\n",
                "        out_shape = out_ab.shape[2:]\n",
                "        orig_shape = tens_orig_l.shape[2:]\n",
                "        \n",
                "        # resize the model prediction to original image size using interpolate\n",
                "        if(out_shape!=orig_shape):\n",
                "            out_ab_orig = F.interpolate(out_ab, size=orig_shape, mode='bilinear')\n",
                "        \n",
                "        concat = torch.cat((tens_orig_l, out_ab_orig), dim=1) # [batch_size, 3, height, width]\n",
                "        return concat \n",
                "    \n",
                "    '''\n",
                "    This function takes in an image and resizes it to 256 by 256\n",
                "    '''\n",
                "    def resize_img(self, img):\n",
                "        return (img.resize((self.height, self.width)))\n",
                "    \n",
                "    '''\n",
                "    Selects random start point in the dataset and prints 7 images\n",
                "    Inputs:\n",
                "        color-> 1 to print color images, 0 for grayscale images\n",
                "    '''\n",
                "    def print_samples(self):\n",
                "        figure, axes = plt.subplots(1, 7, figsize=(18,10))\n",
                "        axes = axes.flatten()\n",
                "        max_idx = self.size if self.size!=None else len(self.paths)\n",
                "        i = random.randint(0, max_idx)\n",
                "        for axis in axes:\n",
                "            l, ab = self[i]\n",
                "            img = self.rebuild_image(l,ab)\n",
                "            axis.imshow(img)\n",
                "            axis.set_xlabel(i)\n",
                "            i+=1\n",
                "        plt.show()\n",
                "    \n",
                "    '''\n",
                "    prints all three LAB channels of a given image index.\n",
                "    also demonstrates how to rebuild original image from l,a,b channels\n",
                "    '''\n",
                "    def print_lab_channels(self,index):\n",
                "        img_l, img_ab = self[index]\n",
                "        \n",
                "        img_lab = torch.concat((img_l,img_ab), dim=0).permute(1,2,0)\n",
                "        l = torch.Tensor(img_l)\n",
                "        a = torch.Tensor(img_ab[0,:,:])[None,:,:]\n",
                "        b = torch.Tensor(img_ab[1,:,:])[None,:,:]\n",
                "        \n",
                "        figure, axes = plt.subplots(1, 5, figsize=(18,10))\n",
                "        axes = axes.flatten()\n",
                "        \n",
                "        # Plot lab iamge and all 3 channels seperately\n",
                "        axes[0].imshow(img_lab) # currently weird colors because img_lab contains negative values\n",
                "        axes[0].set_xlabel(\"LAB image\")\n",
                "        axes[1].imshow(l.permute(1,2,0),cmap=\"gray\")\n",
                "        axes[1].set_xlabel(\"L Channel\")\n",
                "        axes[2].imshow(a.permute(1,2,0))\n",
                "        axes[2].set_xlabel(\"A Channel\")\n",
                "        axes[3].imshow(b.permute(1,2,0))\n",
                "        axes[3].set_xlabel(\"B Channel\")\n",
                "        \n",
                "        orig = self.rebuild_image(img_l,img_ab)\n",
                "        axes[4].imshow(orig)\n",
                "        axes[4].set_xlabel(\"Rebuilt image\")\n",
                "        \n",
                "        plt.tight_layout()\n",
                "        plt.show()\n",
                "\n",
                "    '''\n",
                "    takes an index and prints the original plus the L channel image\n",
                "    side by side\n",
                "    '''\n",
                "    def print_side_by_side(self,i):\n",
                "        l,ab = self[i]\n",
                "        \n",
                "        img = self.rebuild_image(l,ab)\n",
                "        l = l.permute(1,2,0) # original size l\n",
                "\n",
                "        figure, axes = plt.subplots(1, 2, figsize=(18,10))\n",
                "        axes = axes.flatten()\n",
                "        axes[0].imshow(img)\n",
                "        axes[1].imshow(l, cmap=\"gray\")\n",
                "        plt.tight_layout()\n",
                "        plt.show()\n",
                "    \n",
                "    '''\n",
                "    rebuilds original rgb image from l,ab inputs\n",
                "    '''\n",
                "    def rebuild_image(self, l,ab):\n",
                "        return color.lab2rgb(torch.cat((l,ab), dim=0).permute(1,2,0))\n",
                "    \n",
                "    '''\n",
                "    Calculates average width and height of all images in dataset\n",
                "    '''\n",
                "    def calc_average_dimension(self):\n",
                "        totalw = 0\n",
                "        totalh = 0\n",
                "        for i in range(len(self.paths)):\n",
                "            image = Image.open(self.paths[i])\n",
                "            w, h = image.size\n",
                "            totalw += w\n",
                "            totalh += h\n",
                "        avgw = totalw//len(self.paths)\n",
                "        avgh = totalh//len(self.paths)\n",
                "        return avgw, avgh"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "1893cc5b-1b21-4ffd-baca-57defbed2aa2",
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "trans = transforms.Compose([transforms.ToTensor()])\n",
                "ImageNet_train_dataset = ImageNet(root=\"./imagenet/train\", transform=trans)\n",
                "ImageNet_eval_dataset = ImageNet(root=\"./imagenet/val\", transform=trans)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "a2ce8d6d-da5c-4da0-b98a-bfb75c30b737",
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "train_dataloader_imagenet = DataLoader(ImageNet_train_dataset, batch_size=32, shuffle=True)\n",
                "eval_dataloader_imagenet = DataLoader(ImageNet_eval_dataset, batch_size=32, shuffle=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c4d7538a-5b00-452b-9705-03698dfba546",
            "metadata": {
                "tags": []
            },
            "source": [
                "### Testing LAB conversion for ImageNet"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bf160915-8d43-4be0-b98c-66cbcd25457a",
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "temp = ImageNet(root=\"./imagenet/train\", transform=trans, size=1000)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a41e5d59-8364-4d38-992d-df5908c8bc29",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "78cde510-164e-42e2-9c6a-dd7fc871bd0d",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "cea582ef-edee-4a54-8f45-00a3b3b2488d",
            "metadata": {
                "tags": []
            },
            "source": [
                "# Class for the mirFlickr dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cb1e0a75-b3c4-4082-a641-c3866415a453",
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# This code was based on the FlowerDataset class we did in Assignment 2\n",
                "class VisionDataset(Dataset):\n",
                "    '''\n",
                "    Constructor \n",
                "    Inputs:\n",
                "        root -> root dir of images,\n",
                "        ext -> optional extension of images, default jpg\n",
                "        size -> optional, crops data size for faster testing\n",
                "    '''\n",
                "    def __init__(self, root, ext='.jpg', size=None, transform=None):\n",
                "        self.root = root\n",
                "        self.paths = glob.glob(f'{root}/*{ext}', recursive=True)\n",
                "        self.dataset = []         # array of tuples (black/white , original)\n",
                "        self.size = size if size else len(self.paths)\n",
                "        self.transform = transform\n",
                "        \n",
                "\n",
                "    '''\n",
                "    return length of dataset\n",
                "    '''\n",
                "    def __len__(self):\n",
                "        return(len(self.paths))\n",
                "    \n",
                "    '''\n",
                "    get the img,label tuple corresponding to index i\n",
                "    '''\n",
                "    def __getitem__(self, i):\n",
                "        s = (self.paths[i])\n",
                "        img = Image.open(s)\n",
                "        if self.transform:\n",
                "            img = self.transform(img)\n",
                "        return img\n",
                "    \n",
                "    \n",
                "    '''\n",
                "    Selects random start point in the dataset and prints 7 images\n",
                "    Inputs:\n",
                "        color-> 1 to print color images, 0 for grayscale images\n",
                "    '''\n",
                "    def print_samples(self, color:int):\n",
                "        figure, axes = plt.subplots(1, 7, figsize=(18,10))\n",
                "        axes = axes.flatten()\n",
                "        i = random.randint(0, self.size-1000)\n",
                "        print(i, self.size)\n",
                "\n",
                "        for axis in axes:\n",
                "            x = self[i]\n",
                "            if color:\n",
                "                axis.imshow(x)\n",
                "            else:\n",
                "                axis.imshow(x,cmap=\"gray\")\n",
                "            label = self.paths[i]\n",
                "            axis.set_xlabel(label)\n",
                "            i+=1\n",
                "        plt.show()\n",
                "    \n",
                "    '''\n",
                "    Calculates average width and height of all images in dataset\n",
                "    '''\n",
                "    def calc_average_dimension(self):\n",
                "        totalw = 0\n",
                "        totalh = 0\n",
                "        for i in range(len(self.paths)):\n",
                "            image = Image.open(self.paths[i])\n",
                "            w, h = image.size\n",
                "            totalw += w\n",
                "            totalh += h\n",
                "        avgw = totalw//len(self.paths)\n",
                "        avgh = totalh//len(self.paths)\n",
                "        return avgw, avgh"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "f5f35b38-9504-4718-9090-d2c99aa3284b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "importing Jupyter notebook from CUnet.ipynb\n",
                        "torch.Size([1, 1, 256, 256])\n",
                        "torch.Size([1, 1, 128, 128])\n",
                        "torch.Size([1, 1, 256, 256])\n",
                        "torch.Size([1, 1, 256, 256])\n",
                        "torch.Size([1, 2, 256, 256])\n"
                    ]
                }
            ],
            "source": [
                "# we have two models\n",
                "#add an import somewhere, we need import import_ipynb\n",
                "from CUnet import CUNet\n",
                "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
                "model = CUNet()\n",
                "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1) #change as needed\n",
                "loss_function = torch.nn.MSELoss() #change as needed\n",
                "EPOCHS = 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "173405d3",
            "metadata": {},
            "outputs": [],
            "source": [
                "from CUnet import CUNet"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "4e8762f8",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "importing Jupyter notebook from eccv16.ipynb\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\johnz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nbformat\\__init__.py:93: MissingIDFieldWarning: Code cell is missing an id field, this will become a hard error in future nbformat versions. You may want to use `normalize()` on your notebooks before validations (available since nbformat 5.1.4). Previous versions of nbformat are fixing this issue transparently, and will stop doing so in the future.\n",
                        "  validate(nb)\n"
                    ]
                },
                {
                    "ename": "AttributeError",
                    "evalue": "'eccv16' object has no attribute 'model'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39meccv16\u001b[39;00m \u001b[39mimport\u001b[39;00m eccv16\n\u001b[0;32m      2\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m model \u001b[39m=\u001b[39m eccv16(device)\n\u001b[0;32m      4\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mSGD(model\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m) \u001b[39m#change as needed\u001b[39;00m\n\u001b[0;32m      5\u001b[0m loss_function \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mMSELoss() \u001b[39m#change as needed\u001b[39;00m\n",
                        "File \u001b[1;32m<string>:6\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, device)\u001b[0m\n",
                        "\u001b[1;31mAttributeError\u001b[0m: 'eccv16' object has no attribute 'model'"
                    ]
                }
            ],
            "source": [
                "from eccv16 import eccv16\n",
                "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
                "model = eccv16(device)\n",
                "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1) #change as needed\n",
                "loss_function = torch.nn.MSELoss() #change as needed\n",
                "EPOCHS = 20"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "db7b7a48-68c6-4d41-ad47-b5925dff3cf8",
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "flickr_dataset = VisionDataset(\"./mirflickr\", ext=\".jpg\")\n",
                "train_dataloader_vision = DataLoader(flickr_dataset, batch_size=32, shuffle=True)\n",
                "validation_dataloader = None\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "962aaf01-83e0-4ae7-85a6-71e88b982aaa",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\johnz\\AppData\\Local\\Temp\\ipykernel_11128\\732020417.py:29: FutureWarning: The use of this function is discouraged as its behavior may change dramatically in scikit-image 1.0. This function will be removed in scikit-image 1.0.\n",
                        "  img = convert(img, np.float32)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<class 'torch.Tensor'>\n",
                        "<class 'torch.Tensor'>\n"
                    ]
                }
            ],
            "source": [
                "#filename = model/type(model)_epoch_avg_validation_loss.pth'\n",
                "\n",
                "\n",
                "def name_to_epoch(name):\n",
                "    _, a = name.split(\"/\")\n",
                "    _, epoch, _ = name.split(\"_\") #name, epoch, avg_l.pth\n",
                "    return(int(epoch))\n",
                "\n",
                "paths = glob.glob(f'model/{type(model).__name__}_*.pth')\n",
                "#only find models that have the same type as the model being trained\n",
                "\n",
                "paths.sort(key = name_to_epoch) #most recent epoch\n",
                "start = 0\n",
                "if len(paths) > 0:\n",
                "    target = paths[-1]\n",
                "    start = name_to_epoch(target)\n",
                "    model.load_state_dict(torch.load(target)) \n",
                "\n",
                "\n",
                "model = model.to(device)\n",
                "\n",
                "for epoch in range(start, EPOCHS):\n",
                "    model.train()\n",
                "    train_loss = 0\n",
                "    for batch_index,(inputs, expected) in enumerate(train_dataloader_imagenet):\n",
                "        optimizer.zero_grad() \n",
                "        inputs = inputs.to(device)\n",
                "        expected = expected.to(device)\n",
                "        outputs = model(inputs)\n",
                "        loss = loss_function(outputs, expected)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        train_loss += loss.item()*len(expected)\n",
                "    \n",
                "    model.eval()\n",
                "    val_loss = 0\n",
                "    for batch_index,(inputs, expected) in enumerate(eval_dataloader_imagenet):\n",
                "        # inputs = torchvision.transforms.ToTensor(train_dataloader_imagenet.resize_image(lightness))\n",
                "        # expected = torchvision.transforms.ToTensor(train_dataloader_imagenet.resize_image(colors))\n",
                "        # inputs.type(torch.float32)\n",
                "        # expected.type(torch.float32)\n",
                "        print(type(inputs))\n",
                "        inputs = inputs.to(device)\n",
                "        expected = expected.to(device)\n",
                "        outputs = model(inputs)\n",
                "        loss = loss_function(outputs, expected)\n",
                "        val_loss += loss.item()*len(expected)\n",
                "    avg_val_loss = val_loss/len(eval_dataloader_imagenet)\n",
                "    filename = f'model/{type(model).__name__}_{epoch}_{avg_val_loss}.pth'\n",
                "    torch.save(model.state_dict(), filename)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c0e02031",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d29846bf-f176-4aa6-9b99-d037ebaa4fa6",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e14d22d2-2be6-4734-b288-c4558e641ff2",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
